{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Email Data \n",
    "\n",
    "Preparing Chris and Sara’s email for the author identification project; it had to do with a feature that was a little too powerful (effectively acting like a signature, which gives an arguably unfair advantage to an algorithm). You’ll work through that discovery process here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #1 If a decision tree is overfit, would you expect the accuracy on a test set to be very high or pretty low?\n",
    "\n",
    "Low. This bug was found when Katie was trying to make an overfit decision tree to use as an example in the decision tree mini-project. A decision tree is classically an algorithm that can be easy to overfit; one of the easiest ways to get an overfit decision tree is to use a small training set and lots of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #2 If a decision tree is overfit, would you expect high or low accuracy on the training set?\n",
    "\n",
    "The accuracy would be very high on the training set, but would plummet once it was actually tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #3 How many training points are there, according to the starter code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_files = os.listdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['email_authors.pkl',\n",
       " 'email_authors_overfit.pkl',\n",
       " 'unix',\n",
       " 'word_data.pkl',\n",
       " 'word_data_overfit.pkl']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Saved 199 bytes.\n",
      "Done. Saved 17578 bytes.\n",
      "Done. Saved 398 bytes.\n",
      "Done. Saved 17390 bytes.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "convert dos linefeeds (crlf) to unix (lf)\n",
    "usage: dos2unix.py \n",
    "\"\"\"\n",
    "\n",
    "for original in pkl_files:\n",
    "    content = ''\n",
    "    outsize = 0\n",
    "    if 'pkl' in original:\n",
    "        with open('data/' + original, 'rb') as infile:\n",
    "            content = infile.read()\n",
    "        with open('data/unix/' + original, 'wb') as output:\n",
    "            for line in content.splitlines():\n",
    "                outsize += len(line) + 1\n",
    "                output.write(line + str.encode('\\n'))\n",
    "\n",
    "        print(\"Done. Saved %s bytes.\" % (len(content)-outsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the features (words) and labels (autors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The words (features) and authors (labels), already largely processed.\n",
    "### These files should have been created from the previous (Lesson 10)\n",
    "### mini-project.\n",
    "words_file = \"../10_text_learning/your_word_data.pkl\" \n",
    "authors_file = \"../10_text_learning/your_email_authors.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"rb\"))\n",
    "authors = pickle.load( open(authors_file, \"rb\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training/testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test_size is the percentage of events assigned to the test set (the\n",
    "### remainder go into training)\n",
    "### feature matrices changed to dense representations for compatibility with\n",
    "### classifier functions in versions 0.15.2 and earlier\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(word_data, authors, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "# create the tf-idf matrix\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test  = vectorizer.transform(features_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "### a classic way to overfit is to use a small number\n",
    "### of data points and a large number of features;\n",
    "### train on only 150 events to put ourselves in this regime\n",
    "features_train = features_train[:150].toarray()\n",
    "labels_train   = labels_train[:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a decision tree up and training on the training data, and print out the accuracy. How many training points are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 150)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_train), len(labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #1 Create the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(features_train, labels_train, criterion=\"gini\", min_samples_split=2):\n",
    "    \n",
    "    ### returns a trained decision tree classifer\n",
    "    \n",
    "    # instantiate the classifier\n",
    "    clf = tree.DecisionTreeClassifier(criterion=criterion, min_samples_split=min_samples_split)\n",
    "    # fit the data\n",
    "    clf = clf.fit(features_train, labels_train)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the classifier\n",
    "clf = classify(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #2 Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store predictions in a list named pred\n",
    "pred = clf.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #3 Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9476678043230944"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "accuracy = accuracy_score(pred, labels_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #4 What’s the importance of the most important feature? What is the number of this feature?\n",
    "\n",
    "Take your (overfit) decision tree and use the `feature_importances_` attribute to get a list of the relative importance of all the features being used. \n",
    "We suggest iterating through this list (it’s long, since this is text data) and only printing out the feature importance if it’s above some threshold (say, 0.2--remember, if all words were equally important, each one would give an importance of far less than 0.01). \n",
    "\n",
    "In order to figure out what words are causing the problem, you need to go back to the `TfIdf` and use the feature number. You can return a list of all the words in the TfIdf by calling `get_feature_names()` on it; \n",
    "* pull out the word that’s causing most of the discrimination of the decision tree. \n",
    "* What is it? Does it make sense as a word that’s uniquely tied to either Chris Germany or Sara Shackleton, a signature of sorts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07495003, 0.02631579, 0.13402829, 0.76470588])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.feature_importances_[clf.feature_importances_ > 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'000'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature name sshacklensf has the importance 0.7647058823529412 and the number 33614\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0.2:\n",
    "        print(\"Feature name {} has the importance {} and the number {}\".format(vectorizer.get_feature_names()[i], clf.feature_importances_[i], i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This word seems like an outlier in a certain sense, so let’s remove it and refit. Go back to text_learning/vectorize_text.py, and remove this word from the emails using the same method you used to remove “sara”, “chris”, etc. Rerun vectorize_text.py, and once that finishes, rerun find_signature.py. Any other outliers pop up? What word is it? Seem like a signature-type word? (Define an outlier as a feature with importance >0.2, as before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9692832764505119"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the classifier\n",
    "clf = classify(features_train, labels_train)\n",
    "# store predictions in a list named pred\n",
    "pred = clf.predict(features_test)\n",
    "# calculate accuracy\n",
    "accuracy = accuracy_score(pred, labels_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16260163, 0.05060729, 0.66666667, 0.09380863, 0.02631579])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.feature_importances_[clf.feature_importances_ > 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature name cgermannsf has the importance 0.6666666666666667 and the number 14343\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0.5:\n",
    "        print(\"Feature name {} has the importance {} and the number {}\".format(vectorizer.get_feature_names()[i], clf.feature_importances_[i], i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #5 Update `vectorize_test.py` one more time, and rerun. Then run `find_signature.py` again. Any other important features (importance>0.2) arise? How many? Do any of them look like “signature words”, or are they more “email content” words, that look like they legitimately come from the text of the messages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8134243458475541"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the classifier\n",
    "clf = classify(features_train, labels_train)\n",
    "# store predictions in a list named pred\n",
    "pred = clf.predict(features_test)\n",
    "# calculate accuracy\n",
    "accuracy = accuracy_score(pred, labels_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02481019, 0.10537858, 0.02628019, 0.01777778, 0.02552933,\n",
       "       0.04740741, 0.04266667, 0.18692724, 0.36363636, 0.08406921,\n",
       "       0.07551703])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.feature_importances_[clf.feature_importances_ > 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature name houectect has the importance 0.36363636363636365 and the number 21323\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0.2:\n",
    "        print(\"Feature name {} has the importance {} and the number {}\".format(vectorizer.get_feature_names()[i], clf.feature_importances_[i], i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, there is one more word (\"houectect\").  It doesn't look like an obvious signature word so let's keep moving without removing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #6 What’s the accuracy of the decision tree now? We've removed two \"signature words\", so it will be more difficult for the algorithm to fit to our limited training set without overfitting. Remember, the whole point was to see if we could get the algorithm to overfit--a sensible result is one where the accuracy isn't that great!\n",
    "\n",
    "0.8134243458475541\n",
    "Now that we've removed the outlier \"signature words\", the training data is starting to overfit to the words that remain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #7 Use all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9948805460750854"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the classifier\n",
    "clf = classify(features_train, labels_train)\n",
    "# store predictions in a list named pred\n",
    "pred = clf.predict(features_test)\n",
    "# calculate accuracy\n",
    "accuracy = accuracy_score(pred, labels_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
